{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install [modulename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the provided download corpus.py script to download the Text8 corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file exists\n",
      "Loading the dataset...\n",
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "WORKING_DIR = os.getcwd()\n",
    "DATA_DIR = WORKING_DIR + \"/data\"\n",
    "DATA_PATH = DATA_DIR + \"/text8_20m.txt\"\n",
    "DOWNLOAD_CORPUS_SCRIPT = DATA_DIR +\"/download_corpus.py\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(\"Data file exists\")\n",
    "else:\n",
    "    print(\"Data file does not exist. Attempting to download...\")\n",
    "    \n",
    "    os.chdir(DATA_DIR)\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", DOWNLOAD_CORPUS_SCRIPT], # Check if you need to use python3 instead of python\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "    finally:\n",
    "        os.chdir(WORKING_DIR)\n",
    "\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        print(\"Data file downloaded successfully\")\n",
    "    else:\n",
    "        print(\"Failed to download the data file\")\n",
    "\n",
    "# Finally, load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    text = f.read()\n",
    "print(\"Dataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text into words using whitespace as the delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the dataset: 2000000\n",
      "First 10 words in the dataset: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(\"Number of words in the dataset:\", len(words))\n",
    "print(\"First 10 words in the dataset:\", words[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Vocabulary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the frequency of each word in the dataset and keep the top 60000 most frequent, the rest 'UNK' value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king']\n",
      "words_c: [1335, 2861, 12, 6, 193, 1, 4066, 48, 59, 136, 128, 864, 594, 7222, 161, 0, 21681, 1, 0, 88, 851, 3, 0, 14533, 41947, 1, 0, 154, 851, 3549, 0, 193, 10, 181, 59, 4, 6, 11299, 216, 5, 1336, 104, 360, 21, 59, 2795, 372, 5, 2862, 0, 811, 1, 391, 28, 41, 37, 51, 416, 102, 12, 6, 1350, 4151, 17, 529, 792, 2137, 0, 245, 1335, 10, 834, 25, 0, 225, 227, 19107, 2408, 608, 133]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_vocabulary(words, vocabulary_size):\n",
    "    vocabulary = collections.Counter(words).most_common(vocabulary_size)\n",
    "    vocabulary_set = set(word for word, _ in vocabulary)\n",
    "    words_sanitized = [word if word in vocabulary_set else \"UNK\" for word in words]\n",
    "\n",
    "    vocabulary.append((\"UNK\", words_sanitized.count(\"UNK\")))\n",
    "    vocabulary = {index: { 'word': word, 'count': count } for index, (word, count) in enumerate(vocabulary)}\n",
    "\n",
    "    word_to_index = {word['word']: i for i, word in vocabulary.items()}\n",
    "    words_index = [word_to_index[word] for word in words_sanitized]\n",
    "    \n",
    "\n",
    "    return vocabulary, words_index\n",
    "\n",
    "VOCABULARY_SIZE = 60000\n",
    "vocabulary, words_index = build_vocabulary(words, VOCABULARY_SIZE)\n",
    "\n",
    "print(\"words: \", words[:80])\n",
    "print(\"words_c:\", words_index[:80])\n",
    "# print(\"Vocabulary:\", vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Training Data for the Skip-Gram Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Skp-Gram Pairs for each element of the words in the indexed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: [(1335, [2861, 12]), (2861, [1335, 12, 6]), (12, [1335, 2861, 6, 193]), (6, [2861, 12, 193, 1]), (193, [12, 6, 1, 4066]), (1, [6, 193, 4066, 48]), (4066, [193, 1, 48, 59]), (48, [1, 4066, 59, 136]), (59, [4066, 48, 136, 128]), (136, [48, 59, 128, 864])]\n"
     ]
    }
   ],
   "source": [
    "C = 2 # Window size\n",
    "\n",
    "def generate_training_data(words_index, C):\n",
    "    training_pair = []\n",
    "    for i, word_index in enumerate(words_index):\n",
    "        context_word_indices = [words_index[j] for j in range(i - C, i + C + 1) if j != i and 0 <= j < len(words_index)]\n",
    "        training_pair.append((word_index, context_word_indices))\n",
    "\n",
    "    return training_pair\n",
    "\n",
    "t_data = generate_training_data(words_index, C)\n",
    "print(\"Training data:\", t_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing for Negative Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Unigram Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram distribution: [4.171022419034171e-09, 1.9674634052047974e-09, 4.00260755154864e-07, 9.70169321529187e-07, 2.0645249331949006e-08, 1.9081771745946262e-06, 1.285409424733801e-09, 9.157886330093264e-08, 6.76545082936423e-08, 2.8357705880351812e-08]\n"
     ]
    }
   ],
   "source": [
    "def generate_unigram_distribution(words_index, vocabulary):\n",
    "    freqs_w = [vocabulary[word_index]['count'] for word_index in words_index]\n",
    "    N = sum(freqs_w)\n",
    "    unigram_distribution = [freq_w / N for freq_w in freqs_w]\n",
    "    return unigram_distribution\n",
    "\n",
    "unigram_distribution = generate_unigram_distribution(words_index, vocabulary)\n",
    "print(\"Unigram distribution:\", unigram_distribution[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smooth the Unigram Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed unigram distribution: [1.777254096045828e-08, 1.0115739295913317e-08, 5.449101386466149e-07, 1.0585302748307073e-06, 5.8977007123427317e-08, 1.7580517431391346e-06, 7.3510392327936466e-09, 1.8026610173557644e-07, 1.4364489201883038e-07, 7.48291739562492e-08]\n"
     ]
    }
   ],
   "source": [
    "def smooth_unigram_distribution(unigram_distribution, alpha):\n",
    "  sum_unigram_distribution = sum(Uw ** alpha for Uw in unigram_distribution)\n",
    "  smoothed_unigram_distribution = [ Uw ** alpha / sum_unigram_distribution for Uw in unigram_distribution]\n",
    "  return smoothed_unigram_distribution\n",
    "\n",
    "ALPHA = 0.75\n",
    "smoothed_unigram_distribution = smooth_unigram_distribution(unigram_distribution, ALPHA)\n",
    "print(\"Smoothed unigram distribution:\", smoothed_unigram_distribution[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write a report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
